<!DOCTYPE html>
<html>
<title>Quant Trading - Monte Carlo</title>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?

family=Montserrat">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-

awesome/4.7.0/css/font-awesome.min.css">
<style>
body, h1,h2,h3,h4,h5,h6 {font-family: "Montserrat", sans-serif}
.mySlides {display: none}
</style>
<head>
  <link rel="shortcut icon" type="image/x-icon" href="https://raw.githubusercontent.com/tattooday/tattooday.github.io/master/img/favicon.ico" />
</head>
<body class="w3-black">



<!-- Navbar -->
<div >
  <div class="w3-bar w3-theme">
    
    <a href="https://tattooday.github.io" class="w3-bar-item w3-button w3-padding-16">HOME</a>
    <a href="https://tattooday.github.io/#recent" class="w3-bar-item w3-button w3-padding-16">RECENT</a>
    <a href="https://tattooday.github.io/#upcoming" class="w3-bar-item w3-button w3-padding-16">UPCOMING</a>
    <a href="https://tattooday.github.io/#about" class="w3-bar-item w3-button w3-padding-16">ABOUT</a>
    <a href="https://tattooday.github.io/#contact" class="w3-bar-item w3-button w3-padding-16">CONTACT</a>
   
  </div>
</div>




<!-- Navbar on small screens (remove the onclick attribute if you want the 

navbar to always show on top of the content when clicking on the links) -->
<div id="navDemo" class="w3-bar-block w3-black w3-hide w3-hide-large w3-hide-

medium w3-top" style="margin-top:46px">
  <a href="https://tattooday.github.io" class="w3-bar-item w3-button w3-

padding-large" >HOME</a>
  <a href="https://tattooday.github.io/#recent" class="w3-bar-item w3-button w3

-padding-large" >RECENT</a>
  <a href="https://tattooday.github.io/#upcoming" class="w3-bar-item w3-button 

w3-padding-large" >UPCOMING</a>
  <a href="https://tattooday.github.io/#about" class="w3-bar-item w3-button w3-

padding-large" >ABOUT</a>
  <a href="https://tattooday.github.io#contact" class="w3-bar-item w3-button w3

-padding-large" >CONTACT</a>
</div>



<!-- Page Content -->
<div class="w3-padding-large" id="main">
  <!-- Header/Home -->
  <header class="w3-container w3-padding-32 w3-center w3-black" id="home">
    <h1 class="w3-jumbo"><span class="w3-hide-small">T.M.</span></h1>
    <p>Half Researcher, Half Developer.</p>
    <img src="/img/background.jpg" alt="background" class="w3-image" width="992" 

height="1108">
  </header>



  <!-- The Main Section -->
  <div class="w3-container w3-white w3-content w3-centre w3-padding-64" 

id="band">
    <h2 class="w3-wide">Quant Trading - Monte Carlo</h2>
    <br>
    <h6 class="w3-opacity"><i>Monte Carlo simulation in trading is nothing but house of cards.

</i></h6>
    <p class="w3-justify">
    Why do I put this picture (see <a href=https://www.explainxkcd.com/wiki/index.php/2048:_Curve-Fitting>here</a> for explanation) right in front of everything? Have I caught your attention? Hopefully I have. These days, blogs on data science topic are dime a dozen. Some blogs teach you how to use decision tree/random forrest to predict the stock price movement, others illustrate the possibility of back-propagation neural network to forecast a bond price. The brutal truth is, most of them are nothing but house of cards.

Monte Carlo, my first thought on these two words is the grand casino, where you meet Famke Janssen in tuxedo and introduce yourself, 'Bond, James Bond'. Indeed, the simulation is named after the infamous casino. It actually refers to the computer simulation of massive amount of random events. This unconventional mathematical method is extremely powerful in the study of stochastic process. Here comes the argument on Linkedin that caught my eyes the other day. "Stock price can be seemed as a <a href=https://en.wikipedia.org/wiki/Wiener_process>Wiener Process</a>. Hence, we can use Monte Carlo simulation to predict the stock price." said a data science blog. Well, in order to be a Wiener Process, we have to assume the stock price is continuous in time. In reality, the market closes. The overnight volatility exists. But that is not the biggest issue here. The biggest issue is, can we really use Monte Carlo simulation to predict the stock price, even a range or its direction?

The author offered a quite interesting idea. As he suggested, the first step was to run as many simulations as possible on <a href=https://en.wikipedia.org/wiki/Stochastic_differential_equation#Use_in_probability_and_mathematical_finance>stochastic differential equations</a> to predict stock price. The goal of simulations was to pick a best fitted curve (in translation, the smallest standard deviation) compared to the historical data. There might exist a hidden pattern in the historical log return. The best fitted curve had the potential to replicate the hidden pattern and reflect it in the future forecast. The idea sounds neat, doesn't it? Inspired by his idea, we can build up the simulation accordingly. To fully unlock the potential of Monte Carlo simulation on fat tail events, the ticker we pick is General Electric, one of the worst performing stock in 2018. The share price of GE plunged 57.9% in 2018 thanks to its long history of M&A failures. The time horizon of the data series is 2016/1/15-2019/1/15. We split the series into halves, the first half as training horizon and the second half as testing horizon. Monte Carlo simulation will only justify its power if it can predict an extreme event like this.

Let's take a look at the figure below. Wow, what a great fit! The best fit is almost like running a cool linear regression with valid input. As you can see, it smoothly fits the curve in training horizon.

![alt text](https://raw.githubusercontent.com/tattooday/quant-trading/master/Monte%20Carlo%20project/preview/ge%20simulation.png)

If we extend it to testing horizon...

![alt text](https://raw.githubusercontent.com/tattooday/quant-trading/master/Monte%20Carlo%20project/preview/ge%20versus.png)

Oops, house of cards collapses. The real momentum is completely the opposite to the forecast, let alone the actual price. The forecast looks quite okay for the first 50 days of testing horizon. After that, the real momentum falls like a stone down through the deep sea while the forecast is gently climbing up. You may argue the number of the iterations is too small so we cannot make a case. Not exactly, let's look at the figure below.

![alt text](https://raw.githubusercontent.com/tattooday/quant-trading/master/Monte%20Carlo%20project/preview/ge%20accuracy.png)

We start from 500 times of simulation to 1500 times of simulation. Each round we increase the number by 50. We don't look at the actual price forecast, just the direction. If the end price of testing horizon is larger than the end price of training horizon, we define it as gain. Vice versa. Only when both actual and forecast directions align, we consider the forecast is accurate. As the result shows, the prediction accuracy is irrelavant to the numbers of simulation. The accuracy is more sort of tossing a coin to guess heads or tails regardless of the times of simulation. If you think 1500 is still not large enough, you can definitely try 150000, be my guest. We don't have so much computing power as an individual user (frankly no patience is the real reason) but I can assure you the result is gonna stay the same. <a href=https://en.wikipedia.org/wiki/Law_of_large_numbers>Law of Large Numbers</a> theorem would not work here.

Now that the prophet of Monte Carlo turns out to be a con artist. Does Monte Carlo simulation have any application in risk management? Unless you are drinking the Kool-Aid. Let's extend the first figure a little bit longer to the end of testing horizon.

![alt text](https://raw.githubusercontent.com/tattooday/quant-trading/master/Monte%20Carlo%20project/preview/ge%20simulation2.png)

Obviously, out of 500 times of simulation, none of them successfully predict the scale of the downslide. The lowest price of 500 predictions is 10.99 per share. It is still way higher than the real bottom price at 6.71 per share (no wonder GE got its ass kicked out of DJIA). The so-called fat tail event simulation is merely a mirage. If you think GE in 2018 is an extreme case, you'd be so wrong. The next ticker we test is Nvidia from 2006/1/15 to 2009/1/15. In 2008, the share price of Nvidia dropped 75.6%!! The financial crisis is the true playground for risk quants. By default, we continue to split the time horizon of NVDA into two parts by 50:50. The result is in the figure below.

![alt text](https://raw.githubusercontent.com/tattooday/quant-trading/master/Monte%20Carlo%20project/preview/nvda%20versus.png)

Undoubtedly, the best forecast fails the expectation again. It cannot accurately predict the direction of the momentum. For the extreme event, Monte Carlo simulation cannot predict the scale of downslide, again! 6.086 per share is the lowest price we achieve from our 500 times of simulation, yet the actual lowest is at 5.9 per share.

![alt text](https://raw.githubusercontent.com/tattooday/quant-trading/master/Monte%20Carlo%20project/preview/nvda%20simulation.png)

Still thinking about Monte Carlo for your next big project?
    </p>
    <p class="w3-justify">This project is designed to be integrated into my 

scraping script called <a href="https://tattooday.github.io/web-scraping/mena-newsfeed/">
      
MENA newsfeed</a>. Initially, the script would scrape 

news titles from different mainstream websites (so-called fake news, lol) 

including BBC, CNN, Reuters, Al Jazeera etc. All these scraped news titles, 

links and preview images would be concatnated into one HTML email and 

automatically send it to my inbox every morning. After a couple of days, I 

realized that many websites were actually reporting the same story but in 

different titles and preview images. It was totally a waste of energy and time 

to read these similar contents over and over again. And not every piece of 

information was worth my time to read. Some stories such as 'British Iranian 

woman got put in jail' was not exactly business related (I'm sorry, it is still 

a great story though). Couldn't I find a way to create a filter to extract the 

key information?</p>
    <br>
    <img src='https://raw.githubusercontent.com/tattooday/graph-

theory/master/Text%20Mining%20project/preview/email.PNG' class="w3-image" 

width="600" height="450">
    <p class="w3-justify">The first thing came to my mind, of course to anyone, 

was machine learning. We could build up a classifier to determine which one is 

key information. A simple Naive Bayes Multivariate Event Model would do the 

trick. The methodology is basically based on the assumption that when certain 

key words such as 'oil','crude','south pars' and 'LNG' come together in a title, 

the title is more likely to be oil business related. It turned out that 

Bernoulli Naive Bayes Classifier worked pretty well for key information 

screening. The accuracy was always capped at 70 to 80 percent. It could be 

improved by building up a better stopword list. The downside of this is the time 

complexity. It is always the issue of supervised learning especially if it is 

generative learning. The default sklearn package needs to calculate the 

conditional probability of each word in the vocabulary list repeatedly when 

making a forecast. In terms of the execution speed, the whole process could be 

boosted with a little computer science technique called memoization. I did write 

a self implementation of 
      
<a href="https://tattooday.github.io/machine-learning/naive-bayes-optimization/">
  
Naive Bayes</a> that its time complexity was greatly 

optimized. Still, that didn't solve the problem 

of similar contents with different titles. You may argue that I wasn't working 

hard enough. We could build up a two dimensional dataframe with the length of 

n*(n-1)/2 assuming n is how many titles we have scraped. Yes, we could, and we 

convert the dataframe into a multi-dimensional vocabulary matrix. Support Vector 

Machine or Random Forrest or any other classifiers could help us to remove the 

similar contents and extract the key information. Well, our time and space 

complexity issue still exists. We still have to manually classify everything and 

save a training dataset on a local drive.</p>
    <p class="w3-justify">Is that the only trick up my sleeves? Nope, what about 

graph theory? We could connect news titles from different sources together on 

the criteria of how many words they have in common. As usual for natural 

language processing, we always need to keep a stopword list to exclude some stop 

words. In our case, some country names such as 'saudi arabia' or 'iran' or 

'tehran' should be included in the stopword list. Unfortunately, the node names 

of a graph structure cannot be the exact news titles. We do not want to have a 

node name consists of more than fifteen words. In this case, the index of the 

news title in a concatnated dataframe would be presented as the node name. The 

edge between two nodes would be established if and only if two news titles share 

some words in common. The number of common words would be denoted as the weight 

of the edge. But there is another problem, how can we connect word 'walking' 

with word 'walked'? Well, in that case, we also need to include a stemming 

process. Currently NLTK is the most popular tool for text mining. Even though 

the famous Porter stemmer is not very effective, as it's a rule based stemmer 

instead of dictionary based. English, is a terrible language for its messy 

vocabulary rules. Keeping a dictionary based stemmer would require a lot of 

space. Hence, we would have to cope with the imperfect Porter stemmer.</p>
    <p class="w3-justify">Let's visualize the graph network and see what is 

really happening. In the following figure, the color of the edge reflects the 

weight of the edge. The more words two news titles share in common, the warmer 

the edge color is. Some titles may not have any word in common with the rest of 

the titles. These titles do not appear in this graph structure. Later on, we 

could add these rebellious titles back to our output. For the moment, we are 

only concerned with the nodes in the graph ADT.</p>
    <br>
    <img src='https://raw.githubusercontent.com/tattooday/graph-

theory/master/Text%20Mining%20project/preview/original.png' class="w3-image" 

width="600" height="450">
    <p class="w3-justify">Let's look at the assumption of this approach and find 

out why the graph structure is going to work. Assuming a piece of story is a 

breaking news, it is so important that every mainstream media would cover it. 

Maybe different media websites cover the story from a different perspective but 

at least they produce similar content just in relatively different titles. The 

script called MENA newsfeed scrapes a lot of websites. Hence, there must be some 

similar contents with different titles from different sources. These titles 

should be connected to each other as the similar content always should have at 

least one key information in common. This assumption may sound very confusing. 

Looking at this example, we have the following titles, 'Airstrike on Yemen 

school bus is apparent war crime' from CNN, 'UN accuses Saudi coalition of war 

crimes in Yemen' from Financial Times, 'Mistakes admitted in Yemen bus attack' 

from BBC. These titles are connected by the common words 'school bus' and 'war 

crime' ('yemen' is a stop word so not included). The content 'yemen bus attack 

is a war crime' exists in every media website with a different title. We only 

want to see this content once in our email instead of three times from different 

sources. The title of the content with the most common words with others is 

denoted as our target.</p>
    <br>
    <img src='https://raw.githubusercontent.com/tattooday/graph-

theory/master/Text%20Mining%20project/preview/edge.png' class="w3-image" 

width="600" height="450">
    <p class="w3-justify">In a graph structure, our targets could be presented 

as key nodes of strongly connected components. In the following visualization, 

black squares highlight strongly connected components and red circles highlight 

key nodes which either have the most edges in a given strongly connected 

component or connect to other strongly connected components.</p>
    <br>
    <img src='https://raw.githubusercontent.com/tattooday/graph-

theory/master/Text%20Mining%20project/preview/target.png' class="w3-image" 

width="600" height="450">
    <p class="w3-justify">Is there any known traversal algorithm that can return 

a list of our target nodes? Not to my knowledge (If you do, please feel free to 

comment). Nevertheless, our selection criteria is not complex and we could 

always implement our own version of traversal algorithm. The first thing that 

came to my mind was <a href="https://github.com/tattooday/graph-

theory/blob/master/BFS%20DFS%20on%20DCG.ipynb">Breadth First Search</a>. 
      
Think of target nodes as parent 

nodes, all we need to examine is that parent nodes are the nodes with the most 

edges in any given strong connected component. This BFS, I call it Alternative 

BFS, would start at each node in the graph structure. Each starting node is 

defined as a parent node. It would go one layer deeper to the child nodes. Each 

traversal from the parent node to all child nodes returns a tree structure. The 

tree structure would be denoted as a strongly connected component. The Alter BFS 

is designed to select a node with most edges in a given strongly connected 

component and append the node to an output list. When two nodes have the same 

number of edges, the algorithm would look at the total sum of weights of each 

node's edges. The node with highest sum of weights would go to the output list. 

If the sum of weights cannot select a winner, it is then on a first come first 

served basis. Whichever node the algorithm travels first would be selected. In 

the following graph, red nodes are the nodes selected by Alter BFS. 

Unfortunately, the visualization layout of networkx is random unless we specify 

the fixed position for each node (which implies a lot of work). I tried my very 

best to keep all nodes in consistent positions throughout these figures.</p>
    <br>
    <img src='https://raw.githubusercontent.com/tattooday/graph-

theory/master/Text%20Mining%20project/preview/bfs.png' class="w3-image" 

width="600" height="450">
    <p class="w3-justify">Despite the fact that this is an elegant traversal 

algorithm, we could tell that some nodes are still redundant. Even if we use 

set() function to remove duplicates from our output list, we could still end up 

with a lot of false positive. For instance, we have node alpha, beta and gamma. 

Node alpha connects to node beta and others. Node alpha has 4 edges and node 

beta only has 2 edges. Node beta has two edges, which connects to node alpha and 

node gamma.And node gamma only has one edge connected to node beta. So when 

Alter BFS runs on node alpha and node beta, we only keep node alpha in check. 

When Alter BFS runs on node gamma, as node gamma is not connected to node alpha 

and node beta has more edges than node gamma, we would also append node beta in 

the output list. But node beta and node alpha are connected and apparently node 

alpha is the target node we defined. Thus, we need another round of iteration to 

remove one node from any two connected nodes in the output list. </p>
    <br>
    <img src='https://raw.githubusercontent.com/tattooday/graph-

theory/master/Text%20Mining%20project/preview/alpha%2Cbeta%2Cgamma.png' 

width="600" height="450">
    <p class="w3-justify">The methodology is pretty much the same as Alter BFS. 

First, we check if two nodes are connected. If so, we compare the number of 

edges for each node and remove those which have smaller number of edges. If both 

nodes have the same number of edges, the total sum of weights of each node would 

be the selection criteria. Again, if the total sum of weights cannot tell a 

winner, it is always on a first come first served basis.</p>
    <p class="w3-justify">Voila! This is the result of our graph theory based 

text mining! Unlike supervised learning, the algorithm doesn't require an extra 

large memory for training dataset. It beats machine learning in both time and 

space complexity.</p>
    <br>
    <img src='https://raw.githubusercontent.com/tattooday/graph-

theory/master/Text%20Mining%20project/preview/result.png' class="w3-image" 

width="600" height="450">
    <p class="w3-justify">Even though the graph traversal algorithm has given us 

the key information, there is still work to do. For those nodes not included in 

our graph structure, they may be some niche information which is exclusive to 

one particular website. It is best to append them to the output list. 
</p>
    <p class="w3-justify">Finally, is this approach flawless? Nope, no algorithm 

is perfect. As much as I love about the algorithm I developed, Alter BFS could 

still miss some information from the graph structure. Consider we have two 

nodes, 'Nazanin Zaghari-Ratcliffe back in Tehran prison' and 'Some 400 prisoners 

escape prison in Tripoli chaos'. They are connected by the common word 'prison' 

but they appear to be completely different contents. I suppose there is always a 

tradeoff among different algorithms. Is there any room for improvement? Yes and 
      
always. We could implement a heuristic technique as in <a href="https://github.com
  
/tattooday/graph-theory/blob/master/a_star%20maze.ipynb">Graph Theory - A*<a>. 
  
In our alternative BFS traversal, our condition would be the number of edges plus 
      
the total sum of the weights instead of the number of edges alone.</p> 
      
<p>In terms of time, space and accuracy, 

apparently graph structure traversal is a better approach for this scenario 

rather than machine learning. It answers the question, machine learning is not 

the golden solution to everything. Graph theory also has the potential to solve 

sophisticated text mining problems as well.
</p>
    <br>
    <br>
    <p>Click the icon below to be redirected to GitHub Repository</p>
    <a href="https://github.com/tattooday/graph-theory/tree/master/Text

%20Mining%20project" target="_blank"><i class="fa fa-github-alt fa-fw w3-

xxlarge"></i></a>
  <!-- End Main Section -->   
  </div>	



  <hr>  
  <!-- Footer -->
  <footer class="w3-container w3-card w3-black w3-margin-bottom">
    <a href="https://github.com/tattooday" target="_blank"><i class="fa fa-

github-square w3-hover-opacity"></i></a>
    <i class="fa fa-linkedin-square w3-hover-opacity"></i>
    <i class="fa fa-facebook-square w3-hover-opacity"></i>
    <i class="fa fa-twitter-square w3-hover-opacity"></i>
    <i class="fa fa-whatsapp w3-hover-opacity"></i>
    <i class="fa fa-snapchat-square w3-hover-opacity"></i>
    <p class="w3-small">I can change the footer, but this template is really 

fabulous, it is powered by <a href="https://www.w3schools.com/w3css/default.asp" 

target="_blank" class="w3-hover-text-green">w3.css</a></p>
  <!-- End footer -->
  </footer>



<!-- END PAGE CONTENT -->
</div>

</body>
</html>
